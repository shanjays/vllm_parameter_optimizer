import json
import ray
import numpy as np
import time
import torch
import os
import re 
import ast # Python Abstract Syntax Tree
from fast_gym_env import FastGymEnv
from fighter_agent import FighterPilot
from benchmark_worker import BenchmarkWorker

# --- REVISION ---
# The "rogue" ray.init() block that was here has been REMOVED.
# hakt_meta_trainer.py is now the only script that calls ray.init().
# --- END REVISION ---

class HAKT_Reward_Function:
    
    def __init__(self, user_goal, model_name, fast_loop_steps, worker_gpu_id, static_args):
        self.user_goal = user_goal
        self.model_name = model_name
        self.fast_loop_steps = fast_loop_steps
        self.static_args = static_args
        
        print(f"[RewardFn] Requesting BenchmarkWorker for PHYSICAL GPU {worker_gpu_id}")

        self.worker = BenchmarkWorker.options(
            num_gpus=1, # Request 1 GPU from Ray's pool *for the actor process*
        ).remote(worker_gpu_id) # Pass the PHYSICAL ID (7) as an argument
        
        self.initial_state = self._get_initial_state()

    def _clean_non_json_types(self, data):
        """Recursively converts non-JSON serializable types (like sets) to lists."""
        if isinstance(data, dict):
            return {k: self._clean_non_json_types(v) for k, v in data.items()}
        if isinstance(data, list):
            return [self._clean_non_json_types(item) for item in data]
        if isinstance(data, set):
            # Convert set elements recursively and then convert the set itself to a list
            return list(self._clean_non_json_types(list(data))) 
        if data is Ellipsis:
             return "..." # Handle Ellipsis type generated by ast.literal_eval
        return data

    def _get_initial_state(self):
        print("[RewardFn] Getting initial state from worker...")
        try:
            job_id = self.worker.run_fast_gym_benchmark.remote(
                None, self.static_args, {}
            )
            state, reward, csv_data = ray.get(job_id)
            if state is None:
                raise Exception("Worker failed initial profile.")
            print("[RewardFn] Initial state acquired.")
            return state
        except Exception as e:
            print(f"[RewardFn] ERROR: Worker failed initial state check. Using fallback. {e}")
            # Fallback to known "bad" state from our logs
            return np.array([32.3, 40.8, 0.05, 69.9], dtype=np.float32)

    def __call__(self, completions, **kwargs):
        """
        This is the main "reward" entrypoint for the GRPOTrainer.
        'completions' is a list of generated texts (the JSON plans).
        """
        rewards = []
        for i, plan_str in enumerate(completions):
            print(f"\n--- [RewardFn] Processing Mission Plan {i+1}/{len(completions)} ---")
            is_valid_plan = True
            
            try:
                plan = self._extract_json(plan_str)
                
                # --- CRITICAL FIX: Clean the parsed object before serialization ---
                plan = self._clean_non_json_types(plan)
                
                plan_file_path = f"temp_mission_plan_{int(time.time())}_{i}.json"
                with open(plan_file_path, "w") as f:
                    json.dump(plan, f, indent=2) 
                    
                print(f"[RewardFn] Starting 'Fast Loop' PPO training ({self.fast_loop_steps} steps)...")
                top_5_configs = self._run_fast_loop(plan_file_path)
                
                print(f"[RewardFn] Starting 'Slow Gym' validation (Top 5 configs)...")
                final_metric = self._run_slow_gym(top_5_configs)
                
                rewards.append(final_metric)
                
                os.remove(plan_file_path) 
                
            except Exception as e:
                # This catches errors from _extract_json, json.dump, or the fast loop
                is_valid_plan = False
                print(f"[RewardFn] ERROR: HAKT reward calculation failed. Reason: {e}")
                rewards.append(0.0) # Penalize bad/unparseable plans
                
                # If the plan was invalid, we must still run the PPO agent on a default plan
                # to get the PPO agent trained on something (this is handled by the try/except in _run_fast_loop/set_mission_plan)
                if not is_valid_plan:
                    print("[RewardFn] Plan failed. Running Fast Loop on default, penalized plan to ensure training progression.")
                    # Run the Fast Loop on a default plan (which rewards 0.0)
                    default_plan = {
                        'reward_function': {'R_sm_throughput': 0.01}, 
                        'pruned_action_space': {
                            'BLOCK_SIZE_M': [64], 'BLOCK_SIZE_N': [64], 'BLOCK_SIZE_K': [32],
                            'num_warps': [4], 'num_stages': [4]
                        }
                    }
                    default_plan_path = f"temp_default_plan_{int(time.time())}_{i}.json"
                    with open(default_plan_path, "w") as f:
                        json.dump(default_plan, f, indent=2)
                        
                    try:
                        self._run_fast_loop(default_plan_path)
                    except Exception as fe:
                        print(f"[RewardFn] WARNING: Default Fast Loop also failed. {fe}")
                    finally:
                        os.remove(default_plan_path)

        
        return rewards

    def _extract_json(self, llm_output_str):
        """
        Extracts the JSON plan from the LLM's completion using robust hybrid parsing.
        """
        
        # 1. Use regex to find the most likely JSON or Python dictionary block {..}.
        # This is the most stable regex to capture the whole block, ignoring markdown ticks/text.
        # We use '.*' (greedy) inside the braces to capture the entire structure.
        match = re.search(r"(\{.*\})", llm_output_str, re.DOTALL) 
        
        json_str = None
        
        if match:
            json_str = match.group(0).strip() # Capture group 0 is the whole match
        else:
            json_str = None
            
        if json_str:
            # 2. Robust Cleanup (Control characters break both json.loads and ast.literal_eval)
            control_char_re = re.compile(r'[\x00-\x1F\x7F-\x9F]', flags=re.UNICODE)
            
            # Remove markdown ticks that the regex may have included
            cleaned_str = json_str.replace('```json', '').replace('```', '').strip()
            
            # Remove control characters
            cleaned_str = control_char_re.sub('', cleaned_str).strip()
            
            # --- HYBRID PARSING FIX ---
            
            # Try 1: Strict JSON parsing (best for clean output)
            try:
                return json.loads(cleaned_str)
            except json.JSONDecodeError:
                pass # Failed strict parsing, proceed to Try 2

            # Try 2: Python dictionary parsing (best for single quotes, comments, trailing stuff)
            try:
                # --- DEBUGGING OUTPUT FOR SYNTAX ERROR ---
                print(f"DEBUG: Falling back to AST on string: {cleaned_str}")
                # --- END DEBUGGING ---
                return ast.literal_eval(cleaned_str)
            except (SyntaxError, ValueError, json.JSONDecodeError) as e:
                # This catches the ':' expected error.
                # Log the error and the problematic string portion
                print(f"ERROR parsing LLM JSON: {e} --- Failing String: {cleaned_str[:120]}...")
                raise e # Raise the last error encountered

        # If no JSON was found after all attempts
        raise ValueError("No valid JSON structure found in LLM output.")
            
    def _run_fast_loop(self, mission_plan_path):
        """
        Runs the *entire* PPO training loop for the "Fighter Pilot".
        """
        env = FastGymEnv(
            mission_plan_path=mission_plan_path,
            benchmark_worker=self.worker,
            static_args=self.static_args,
            initial_state=self.initial_state
        )
        
        epoch_log_dir = f"./hakt_logs/run_{int(time.time())}/"
        pilot = FighterPilot(env, log_dir=epoch_log_dir)
        
        print(f"[{pilot.__class__.__name__}] Training on {pilot.device} for {self.fast_loop_steps} steps...")
        pilot.train_epoch(steps=self.fast_loop_steps)
        
        top_results = env.get_top_results(n=5)
        
        print(f"[RewardFn] Fast Loop completed. Found {len(top_results)} unique results.")
        
        env.close()
        del pilot
        del env
        
        return top_results

    def _run_slow_gym(self, top_configs_from_la):
        """
        Runs the 'vllm bench' validation on the "Slow Gym" (GPU 1).
        """
        if not top_configs_from_la:
            print("[RewardFn] No valid configurations found for Slow Gym validation.")
            return 0.0 # No valid configs found

        validation_ids = []
        for config_tuple in top_configs_from_la:
            params, state, reward = config_tuple
            job_id = self.worker.run_slow_gym_validation.remote(
                params, self.model_name, self.user_goal
            )
            validation_ids.append(job_id)

        print(f"[RewardFn] Awaiting validation metrics for {len(top_configs_from_la)} configs from BenchmarkWorker...")
        validation_metrics = ray.get(validation_ids)
        
        # 3. Find the best metric (the "reward" for the LLM)
        if self.user_goal == "throughput":
            best_metric = max(validation_metrics)
        else: # "latency"
            # Get min non-zero latency
            valid_latencies = [m for m in validation_metrics if m > 0]
            best_metric = min(valid_latencies) if valid_latencies else 0.0 
            # (Note: for latency, GRPOTrainer should be set to *minimize* reward)
        
        print(f"[RewardFn] Slow Gym validation complete. Best metric: {best_metric}")
        return best_metric
